# ğŸ¤– LLM Cluster Deployment on AWS EKS

This project sets up a scalable LLM (Large Language Model) inference infrastructure on **Amazon EKS** using CPU-based instances â€” with a clear path to upgrade for GPU acceleration.

---

## ğŸ“Œ Overview

The deployment architecture includes:

- ğŸ§  **Ollama** â€“ Model serving via StatefulSet (2 replicas)
- ğŸŒ **Open Web UI** â€“ Frontend interface for interaction
- ğŸ”€ **Nginx LLM Router** â€“ For intelligent request routing and rate limiting
- â˜ï¸ **AWS EKS Cluster** â€“ With auto-scaling node groups (On-demand & Spot)
- ğŸ’¾ **Persistent Storage** â€“ For model weights
- ğŸ“Š **Monitoring & Logging Infrastructure**

---

## ğŸ—ºï¸ Architecture Diagram

## Architecture

```
                         +------------+
                         |   User     |
                         +------------+
                               |
                               v
                     +-------------------+
                     |   LLM Router      |
                     |   (Nginx)         |
                     +-------------------+
                       /             \
                      v               v
         +------------------+   +------------------+
         |  Open Web UI     |   |   Ollama API     |
         |  (Frontend)      |   |   (Model Server) |
         +------------------+   +------------------+
                  |                     |
                  v                     v
           +-------------+     +--------------------+
           |   Request   | --> | Model Inference    |
           |   Forward   |     | (Ollama Pod 1 & 2) |
           +-------------+     +--------------------+
                                        |
                                        v
                              +--------------------+
                              | Persistent Storage |
                              |   (PVC Volumes)    |
                              +--------------------+
```

Key Components:
1. **LLM Router**: Nginx reverse proxy handling load balancing
2. **Open Web UI**: User-friendly interface for model interaction
3. **Ollama API**: REST endpoint for model inference
4. **Stateful Pods**: Ollama instances with persistent model storage


âš™ï¸ Prerequisites
âœ… AWS Account with necessary permissions
âœ… Terraform v1.0+
âœ… AWS CLI configured (aws configure)
âœ… kubectl configured
âœ… helm installed

ğŸš€ Deployment Steps
1ï¸âƒ£ Infrastructure Provisioning
bash
Copy
Edit
cd terraform/
terraform init
terraform plan
terraform apply

2ï¸âƒ£ Deploy Kubernetes Resources
kubectl apply -f storage-class.yaml
kubectl apply -f ollama-pvc.yaml
kubectl apply -f ollama-statefulset.yaml
kubectl apply -f openwebui-deployment.yaml
kubectl apply -f llm-router.yaml

ğŸ§© Configuration Details
ğŸ›  Terraform Modules
EKS Cluster: Kubernetes v1.33

Node Groups:

On-demand: 2 Ã— c6a.8xlarge (32 vCPUs, 64GB RAM)
Spot: 2 Ã— c6i.4xlarge / c6a.4xlarge (16 vCPUs, 32GB RAM)
Networking: VPC with public/private subnets across 3 AZs
Storage: 60GB gp2 volumes for model weights


ğŸ³ Kubernetes Components
Component	Description
Ollama	StatefulSet, 2 replicas, anti-affinity rules
CPU: 6â€“8 cores, Memory: 12â€“16 GB
Open Web UI	Single replica + LoadBalancer service
LLM Router	Nginx with rate limiting (10 req/sec)


ğŸ§  Default Model: llama3.1:8b

âš¡ GPU Configuration (Theoretical Support)
To enable GPU support, update the following:

1ï¸âƒ£ Terraform (eks.tf)
hcl
Copy
Edit
eks_managed_node_groups = {
  gpu_nodegroup = {
    name           = "gpu-nodegroup"
    instance_types = ["g5.2xlarge"]
    capacity_type  = "ON_DEMAND"
    min_size       = 1
    max_size       = 2
    desired_size   = 1

    labels = {
      accelerator = "nvidia"
    }

    taints = [{
      key    = "nvidia.com/gpu"
      value  = "present"
      effect = "NO_SCHEDULE"
    }]
  }
}

2ï¸âƒ£ Kubernetes Ollama Deployment Changes
Install NVIDIA plugin:

bash
Copy
Edit
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml
Update Ollama container spec:

yaml
Copy
Edit
resources:
  limits:
    nvidia.com/gpu: 1
env:
  - name: CUDA_VISIBLE_DEVICES
    value: "all"

nodeSelector:
  accelerator: "nvidia"
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"


ğŸ“ˆ Performance Benchmarks (CPU-only)
Model: llama3.1:8b
Prompt: "Explain quantum computing in one sentence."
Requests: 100 | Concurrency: 10

ğŸ” Test 1
ğŸ“Š Results (N=100):
âœ… Success Rate: 25.0%
â±ï¸ Latency (ms):
  - Avg: 16780.62
  - P50: 21698.38
  - P90: 30069.23
  - P99: 30118.89
  - Max: 30119.01

ğŸ” Test 2
ğŸ“Š Results (N=100):
âœ… Success Rate: 82.0%
â±ï¸ Latency (ms):
  - Avg: 21061.50
  - P50: 21554.29
  - P90: 30057.99
  - P99: 30115.88
  - Max: 30182.28

ğŸ” Test 3
ğŸ“Š Results (N=100):
âœ… Success Rate: 63.0%
â±ï¸ Latency (ms):
  - Avg: 21733.91
  - P50: 27783.08
  - P90: 30109.54
  - P99: 30138.11
  - Max: 30146.72


## ğŸ§® Estimated GPU Performance

| **Metric**        | **CPU (c6a.8xlarge)** | **GPU (g5.2xlarge â€“ A10G)**     |
|-------------------|-----------------------|----------------------------------|
| Avg Latency       | ~16,780 ms            | ~839 ms                          |
| Throughput        | ~0.59 req/s           | ~11.91 req/s                     |
| Cost/hr           | ~$1.20                | ~$1.51                           |
| Energy Efficiency | 1x                    | ~15â€“20x                          |


ğŸ§¯ Troubleshooting

â— Model Not Loading
Check PVC storage usage
Ensure outbound internet access for model downloads

â— High Latency
Scale Ollama replicas
Increase node size/resources

â— GPU Issues
Confirm NVIDIA plugin status
Check for driver/device plugin logs

ğŸš§ Known Limitations

Only CPU-based instances are used due to:
AWS GPU quota limits
High GPU cost
Additional driver complexity
Large models (>8B) are not supported without GPUs

ğŸŒŸ Future Improvements
 Enable GPU-backed node group
 Autoscaling based on queue length
 Model cache & LRU strategy
 Prometheus/Grafana metrics dashboard

ğŸ¥ Video Demo
ğŸ“½ï¸ [[text](https://drive.google.com/file/d/1v-97KfXKRvpOTaDp5BEqhk28EtaHbjH5/view?usp=sharing)]

ğŸ™Œ Contributions & Feedback
Have ideas to improve this project? Open an issue or submit a PR!

